{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline: PDF to FAISS Vector Store\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load and chunk a PDF document\n",
    "2. Create embeddings and store them in FAISS\n",
    "3. Save the index locally as .faiss and .pkl files\n",
    "4. Load the saved index and create a retriever for RAG applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "Run this cell first to install all necessary dependencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import List, Optional\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Set your PDF path and output directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Path: ../data/dc2523af.pdf\n",
      "Index Save Directory: ../faiss_index\n",
      "Embedding Model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "PDF_PATH = \"../data/dc2523af.pdf\"  # Change this to your PDF path\n",
    "INDEX_SAVE_DIR = \"../faiss_index\"  # Directory to save the FAISS index\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Embedding model to use\n",
    "\n",
    "# Text splitting parameters\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "print(f\"PDF Path: {PDF_PATH}\")\n",
    "print(f\"Index Save Directory: {INDEX_SAVE_DIR}\")\n",
    "print(f\"Embedding Model: {EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "model_name = \"gpt-4o\"\n",
    "llm = ChatOpenAI(\n",
    "                model=model_name,\n",
    "                #openai_api_base=openai_api_base\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading embedding model...\n",
      "Components initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(\"Components initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Process PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from ../data/dc2523af.pdf...\n",
      "Loaded 69 pages from PDF\n",
      "First page preview (first 200 chars): ...\n"
     ]
    }
   ],
   "source": [
    "# Load PDF\n",
    "print(f\"Loading PDF from {PDF_PATH}...\")\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(pages)} pages from PDF\")\n",
    "print(f\"First page preview (first 200 chars): {pages[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents into chunks\n",
      "Created 130 chunks\n",
      "First chunk preview: 1 Introduction ........................................................................................  5\n",
      "1.1 Ownership of this document 5\n",
      "1.2\t API\tDefinition\tand\tOverview\t 5\n",
      "1.3 Purpose 6\n",
      "1.4 Scope ...\n",
      "Average chunk length: 767 characters\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "print(\"Splitting documents into chunks\")\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(f\"First chunk preview: {chunks[0].page_content[:200]}...\")\n",
    "print(f\"Average chunk length: {sum(len(chunk.page_content) for chunk in chunks) // len(chunks)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created successfully!\n",
      "Index contains 130 vectors\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS vectorstore\n",
    "print(\"Creating FAISS index\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "print(\"FAISS index created successfully!\")\n",
    "print(f\"Index contains {vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Index Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving FAISS index to ../faiss_index...\n",
      "Index saved successfully!\n",
      "Files created: ['index.faiss', 'index.pkl']\n"
     ]
    }
   ],
   "source": [
    "# Create directory if it doesn't exist\n",
    "os.makedirs(INDEX_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save FAISS index locally\n",
    "print(f\"Saving FAISS index to {INDEX_SAVE_DIR}...\")\n",
    "vectorstore.save_local(INDEX_SAVE_DIR)\n",
    "\n",
    "print(\"Index saved successfully!\")\n",
    "\n",
    "# List files in the directory\n",
    "files = os.listdir(INDEX_SAVE_DIR)\n",
    "print(f\"Files created: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Saved Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS index from ../faiss_index\n",
      "Index loaded successfully!\n",
      "Loaded index contains 130 vectors\n"
     ]
    }
   ],
   "source": [
    "# Load the saved FAISS index\n",
    "print(f\"Loading FAISS index from {INDEX_SAVE_DIR}\")\n",
    "loaded_vectorstore = FAISS.load_local(\n",
    "    INDEX_SAVE_DIR, \n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(\"Index loaded successfully!\")\n",
    "print(f\"Loaded index contains {loaded_vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Retriever for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever created successfully!\n",
      "Retriever configured to return top 4 most similar documents\n"
     ]
    }
   ],
   "source": [
    "# Create retriever\n",
    "retriever = loaded_vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # Number of documents to retrieve\n",
    ")\n",
    "\n",
    "print(\"Retriever created successfully!\")\n",
    "print(f\"Retriever configured to return top 4 most similar documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing retriever with query: 'What is the main topic of this document?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kh/khjtwkx55nz26qv154hsd08h0000gn/T/ipykernel_63279/595554248.py:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(test_query)\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4 documents\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document 1:\n",
      "Content: contain\tno\texplicit\ttechnology\tor\tprotocol\trestrictions;\trather,\tthe\tdocument\toffers\tbest\tpractices-\n",
      "based\tguidelines\tthat\tensure\tthat\tUAE\tdigital\tgovernment\tAPIs\tare\teffective,\tdesigned\tcorrectly,\t\n",
      "secure\tand\tprovide\tvalue.\n",
      "1.3.\tPurpose...\n",
      "Metadata: {'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 15.0 (Macintosh)', 'creationdate': '2021-04-29T13:43:36+04:00', 'moddate': '2021-05-02T11:10:50+04:00', 'trapped': '/False', 'source': '../data/dc2523af.pdf', 'total_pages': 69, 'page': 7, 'page_label': '7'}\n",
      "------------------------------\n",
      "\n",
      "Document 2:\n",
      "Content: UAE Government API First Guidlines\n",
      "4\n",
      "1.\n",
      "Ownership\n",
      "of this document\n",
      "Introduction\n",
      "1.1....\n",
      "Metadata: {'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 15.0 (Macintosh)', 'creationdate': '2021-04-29T13:43:36+04:00', 'moddate': '2021-05-02T11:10:50+04:00', 'trapped': '/False', 'source': '../data/dc2523af.pdf', 'total_pages': 69, 'page': 4, 'page_label': '4'}\n",
      "------------------------------\n",
      "\n",
      "Document 3:\n",
      "Content: 1.Introduction\n",
      "7\n",
      "The\tpurpose\tof\tthis\tdocument\tis\tto\tprovide\tAPI\tguidelines\tfor\tUAE\tgovernment\tentities.\t\n",
      "The UAE\tgovernment\taims\tto\tadopt\tan\tAPI-first\tapproach\tfor\tthe\tdigital\ttransformation\tinitiatives.\t\n",
      "Government\tentities\tand\tvendors\tcan\tfollow\tthe\tAPI\tguidelines\toutlined\tin\tthis\tdocument\tfor\tgui...\n",
      "Metadata: {'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 15.0 (Macintosh)', 'creationdate': '2021-04-29T13:43:36+04:00', 'moddate': '2021-05-02T11:10:50+04:00', 'trapped': '/False', 'source': '../data/dc2523af.pdf', 'total_pages': 69, 'page': 7, 'page_label': '7'}\n",
      "------------------------------\n",
      "\n",
      "Document 4:\n",
      "Content: Documentation \n",
      "Requirements Description\n",
      "Information Handling, Incident \n",
      "Management and Risk \n",
      "Management\n",
      "Availability, Ownership and \n",
      "Depreciation Policies\n",
      "Governance Frameworks e.g\tPCI\tcompliance\tfor\tPayment\tAPI\n",
      "API Lifecycle Management\n",
      "The\tguidelines\tshould\tdefine\tpolicies\tfor\tAPI\tLifecycle\tmanagem...\n",
      "Metadata: {'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 15.0 (Macintosh)', 'creationdate': '2021-04-29T13:43:36+04:00', 'moddate': '2021-05-02T11:10:50+04:00', 'trapped': '/False', 'source': '../data/dc2523af.pdf', 'total_pages': 69, 'page': 64, 'page_label': '64'}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the retriever with a sample query\n",
    "test_query = \"What is the main topic of this document?\"  \n",
    "\n",
    "print(f\"Testing retriever with query: '{test_query}'\")\n",
    "results = retriever.get_relevant_documents(test_query)\n",
    "\n",
    "print(f\"Retrieved {len(results)} documents\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content[:300]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Utility Functions for Reusability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_vectorstore(pdf_path: str, index_dir: str, force_recreate: bool = False):\n",
    "    \"\"\"\n",
    "    Load existing vectorstore or create new one if it doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        index_dir: Directory for FAISS index\n",
    "        force_recreate: Whether to recreate index even if it exists\n",
    "    \n",
    "    Returns:\n",
    "        FAISS vectorstore object\n",
    "    \"\"\"\n",
    "    if not force_recreate and os.path.exists(index_dir) and os.listdir(index_dir):\n",
    "        print(f\"üìÇ Loading existing index from {index_dir}\")\n",
    "        return FAISS.load_local(\n",
    "            index_dir, \n",
    "            embeddings, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(f\"üîÑ Creating new index from {pdf_path}\")\n",
    "        # Load and process PDF\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pages = loader.load()\n",
    "        chunks = text_splitter.split_documents(pages)\n",
    "        \n",
    "        # Create and save vectorstore\n",
    "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "        os.makedirs(index_dir, exist_ok=True)\n",
    "        vectorstore.save_local(index_dir)\n",
    "        \n",
    "        return vectorstore\n",
    "\n",
    "def create_retriever(pdf_path: str, index_dir: str, k: int = 4, force_recreate: bool = False):\n",
    "    \"\"\"\n",
    "    Create a retriever from PDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        index_dir: Directory for FAISS index\n",
    "        k: Number of documents to retrieve\n",
    "        force_recreate: Whether to recreate index\n",
    "    \n",
    "    Returns:\n",
    "        LangChain retriever object\n",
    "    \"\"\"\n",
    "    vectorstore = load_or_create_vectorstore(pdf_path, index_dir, force_recreate)\n",
    "    return vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": k}\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Utility functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Example: Using the Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using utility functions\n",
    "# This will load existing index or create new one if needed\n",
    "example_retriever = create_retriever(\n",
    "    pdf_path=PDF_PATH,\n",
    "    index_dir=INDEX_SAVE_DIR,\n",
    "    k=3,  # Retrieve top 3 documents\n",
    "    force_recreate=False  # Set to True to force recreation\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Example retriever created!\")\n",
    "\n",
    "# Test with a different query\n",
    "test_query_2 = \"main concepts\"  # Change this for your specific content\n",
    "results_2 = example_retriever.get_relevant_documents(test_query_2)\n",
    "\n",
    "print(f\"\\nüîç Query: '{test_query_2}'\")\n",
    "print(f\"üìÑ Found {len(results_2)} relevant documents\")\n",
    "for i, doc in enumerate(results_2):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "üéâ **Congratulations!** You have successfully:\n",
    "\n",
    "1. ‚úÖ Loaded and chunked a PDF document\n",
    "2. ‚úÖ Created embeddings using HuggingFace models\n",
    "3. ‚úÖ Built a FAISS vector index\n",
    "4. ‚úÖ Saved the index locally (creates .faiss and .pkl files)\n",
    "5. ‚úÖ Loaded the saved index\n",
    "6. ‚úÖ Created a retriever for RAG applications\n",
    "\n",
    "### Files Created:\n",
    "- `index.faiss`: The FAISS index file\n",
    "- `index.pkl`: Metadata and document store\n",
    "\n",
    "### Next Steps:\n",
    "- Integrate this retriever with your RAG chain\n",
    "- Experiment with different embedding models\n",
    "- Try different chunk sizes and overlap values\n",
    "- Add more documents to expand your knowledge base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
